\documentclass[12pt, titlepage]{article}

\usepackage{comment}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amstext}
\usepackage{zed-csp}
\usepackage{pdflscape}
\usepackage{caption}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{placeins}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

% ===== External links (used only inside Glossary) =====
\newcommand{\CatanExt}{\href{https://en.wikipedia.org/wiki/Catan}{Catan}}
\newcommand{\AIExt}{\href{https://en.wikipedia.org/wiki/Artificial_intelligence}{AI}}
\newcommand{\RLExt}{\href{https://www.ibm.com/think/topics/reinforcement-learning}{RL}}
\newcommand{\DigitalTwinExt}{\href{https://en.wikipedia.org/wiki/Digital_twin}{Digital Twin}}
\newcommand{\CVExt}{\href{https://www.ibm.com/think/topics/computer-vision}{CV}}
\newcommand{\LLMExt}{\href{https://www.cloudflare.com/learning/ai/what-is-large-language-model/}{LLM}}
\newcommand{\GameStateExt}{\href{https://milvus.io/ai-quick-reference/what-is-a-state-in-rl}{Game State}}

\newcommand{\NFRExt}{\text{Non-functional Requirement}}
\newcommand{\FRExt}{\text{Functional Requirement}}


\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
10/19/2025 & Draft & Inital VnV plan without Unit Testing\\
... & ... & ...\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage



% ===== Internal glossary link targets =====
\hypertarget{glossary-catan}{}
\hypertarget{glossary-ai}{}
\hypertarget{glossary-rl}{}
\hypertarget{glossary-dt}{}
\hypertarget{glossary-cv}{}
\hypertarget{glossary-llm}{}
\hypertarget{glossary-gamestate}{}
\hypertarget{glossary-nfr}{}
\hypertarget{glossary-fr}{}

% ===== Internal references (used throughout document) =====
\newcommand{\Catan}{\hyperlink{glossary-catan}{Catan}}
\newcommand{\AI}{\hyperlink{glossary-ai}{AI}}
\newcommand{\RL}{\hyperlink{glossary-rl}{RL}}
\newcommand{\DigitalTwin}{\hyperlink{glossary-dt}{Digital Twin}}
\newcommand{\CV}{\hyperlink{glossary-cv}{CV}}
\newcommand{\LLM}{\hyperlink{glossary-llm}{LLM}}
\newcommand{\GameState}{\hyperlink{glossary-gamestate}{Game State}}
\newcommand{\NFR}{\hyperlink{glossary-nfr}{NFR}}
\newcommand{\FR}{\hyperlink{glossary-fr}{FR}}

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}
The following glossary headers are hyperlinked to their online definitions for further reading.
\begin{itemize}
    \item \hypertarget{glossary-catan}{\CatanExt{}} – A strategy board game called \textit{Settlers of Catan} where players collect resources, build roads/settlements, and trade to earn points.[1]
    \item \hypertarget{glossary-ai}{\AIExt{}} – Field of computer science and engineering that focuses on creating systems capable of performing tasks that usually require human intelligence.[2]
    \item \hypertarget{glossary-rl}{\RLExt{}} – A type of machine learning where an agent, known as Reinforcement Learning, learns by interacting with an environment and receiving rewards or penalties for its actions.[3]
    \item \hypertarget{glossary-dt}{\DigitalTwinExt{}} – A digital system that mirrors a physical one. In this project, it refers to a digital representation of the physical \emph{Catan} board, updated in real time.[4]
    \item \hypertarget{glossary-cv}{\CVExt{}} – An area of AI that trains computers to interpret and understand visual information, such as the physical \emph{Catan} board.[5]
    \item \hypertarget{glossary-llm}{\LLMExt{}} – A machine learning model trained on large amounts of text data to generate and understand language.[6]
    \item \hypertarget{glossary-gamestate}{\GameStateExt{}} – The current configuration of the game, including player resources, board layout, and dice rolls.[7]
\end{itemize}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

  The software being tested is RLCatan,
 an AI system that learns to play the board game Catan 
 competitively through reinforcement learning. 
 It includes a learning agent trained in a simulated
  environment, a computer vision module for detecting 
  and interpreting the physical game board, and a
  interface that displays the game state and AI-generated 
  move suggestions to users. The general purpose of RLCatan
  is to provide optimal move suggestions to human players
  based on real-time analysis of the game state.

\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

\wss{You should also list the objectives that are out of scope.  You don't have 
the resources to do everything, so what will you be leaving out.  For instance, 
if you are not going to verify the quality of usability, state this.  It is also 
worthwhile to justify why the objectives are left out.}

\wss{The objectives are important because they highlight that you are aware of 
limitations in your resources for verification and validation.  You can't do everything, 
so what are you going to prioritize?  As an example, if your system depends on an 
external library, you can explicitly state that you will assume that external library 
has already been verified by its implementation team.}

The objective is to build confidence 
in the correctness, reliability, and functional performance of the RLCatan system.
 Verification will focus on ensuring that the reinforcement learning agent generates 
 valid moves, that the computer vision module correctly interprets game states, and 
 that data exchange between system components is accurate and consistent.

\medskip

 Out of scope are extensive usability studies and large-scale performance benchmarking,
 as these exceed the project’s time and resource constraints. Additionally, external
  dependencies such as Catanatron, OpenCV, and YOLOv9 are assumed to be verified by
   their developers, and will only be tested for correct integration within RLCatan.

\subsection{Challenge Level and Extras}

\wss{State the challenge level (advanced, general, basic) for your project.
Your challenge level should exactly match what is included in your problem
statement.  This should be the challenge level agreed on between you and the
course instructor.  You can use a pull request to update your challenge level
(in TeamComposition.csv or Repos.csv) if your plan changes as a result of the
VnV planning exercise.}

\wss{Summarize the extras (if any) that were tackled by this project.  Extras
can include usability testing, code walkthroughs, user documentation, formal
proof, GenderMag personas, Design Thinking, etc.  Extras should have already
been approved by the course instructor as included in your problem statement.
You can use a pull request to update your extras (in TeamComposition.csv or
Repos.csv) if your plan changes as a result of the VnV planning exercise.}

The challenge level for this project is Advanced,
 as outlined in the problem statement and confirmed 
 with the course instructor. This project
  reflects the complexity of developing a 
  reinforcement learning agent capable of
   mastering \textit{Settlers of Catan},
    integrating computer vision for real-time game state 
    analysis, and ensuring seamless interaction between
     system components.

\medskip
The approved extras for this project include:
\begin{itemize}
    \item User Instructional Video: A walkthrough video that explains how to set up, run, and interact with the software, helping users understand its functionality and workflow.
    \item Performance Report: A comprehensive report summarizing the AI's performance metrics, learning progression, and evaluation results based on various test scenarios.
\end{itemize}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (design documents, like MG, MIS, etc).  You
  can include these even before they are written, since by the time the project
  is done, they will be written.  You can create BibTeX entries for your
  documents and within those entries include a hyperlink to the documents.}

\citet{SRS}

\wss{Don't just list the other documents.  You should explain why they are relevant and 
how they relate to your VnV efforts.}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

This section outlines the strategy for Verification and Validation (VnV) across all phases of the RLCatan project lifecycle.
The plan details the roles and responsibilities of the VnV team, and specifies structured approaches for verifying the Software Requirements Specification (SRS), Design, and Implementation, as well as the strategy for Software Validation.
This systematic approach is designed to increase confidence in the correctness, functional performance, and reliability of the final system.
(insert visual roadmap)


\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}


The VnV efforts will be a collaborative team effort, with specific responsibilities assigned based on existing project roles to ensure systematic coverage and accountability.

\begin{table}[h!]
  \centering
  \caption{Verification and Validation Team Roles}
  \label{tab:vnv-team-roles}
  \begin{tabularx}{\textwidth}{|p{2cm}|p{2cm}|X|}
  \hline
  \textbf{Team Member} & \textbf{Project Role} & \textbf{VnV Responsibilities} \\ \hline
  Jake Read & Team Leader & Coordinating all VnV activities, performing final review of all test cases, and leading the code walkthrough for the supervisor. \\ \hline
  Rebecca Di Filippo & Notetaker & Documenting VnV meeting minutes, maintaining traceability tables, and preparing the Usability Survey for validation. \\ \hline
  Sunny Yao & IT/DevOps & Implementing and maintaining CI/CD pipelines, monitoring automated test coverage metrics, and troubleshooting technical VnV tool issues. \\ \hline
  Matthew Cheung & Researcher & Researching and applying non-dynamic testing techniques (e.g., formal code inspection checklists) and collecting external data for software validation. \\ \hline
  Dr. Istvan David & Project Supervisor & Providing technical oversight, expert consultation on RL model correctness, and participating in the Revision 0 demonstration for requirements validation. \\ \hline
  \end{tabularx}
\end{table}

\FloatBarrier


\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
  primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
read over the SRS.  You should explain your structured approach to the review.
Will you have a meeting?  What will you present?  What questions will you ask?
Will you give them instructions for a task-based inspection?  Will you use your
issue tracker?}


The SRS will be verified against the project’s objectives to ensure all requirements are unambiguous, correct, feasible, and complete (Quality System Tests).

\begin{itemize}
    \item \textbf{Structured Review with Supervisor:} A dedicated meeting will be scheduled with Dr. Istvan David, the Project Supervisor.
    The team will present the consolidated functional, non-functional, and safety requirements, specifically focusing on the formalizations (e.g., Z-Notation) and the key constraints (e.g., $\leq 5$ second response time for advice).
    Specific questions will be prepared beforehand to validate the technical accuracy of the $\RL{}$ requirements (e.g., reward function structure).
    \item \textbf{Peer Review and Inspection:} Classmates and primary reviewers will be engaged to perform an ad-hoc review of the SRS, targeting clarity and consistency.
    \item \textbf{Issue Tracker for Traceability:} All feedback and identified issues will be logged on the GitHub Issues board, assigned to a team member, and traced back to the specific requirement for resolution.
    \item \textbf{Checklist-Based Inspection:} The team will employ a custom checklist (developed by the Researcher) focusing on the characteristics of a high-quality requirement (unambiguous, verifiable, complete) to systematically inspect the SRS.
\end{itemize}


\wss{Maybe create an SRS checklist?}

\subsection{Design Verification}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}


Design verification will occur after the Design Document is completed (Milestone: Nov. 10) to ensure the architectural design meets all requirements and is ready for implementation.

\begin{itemize}
    \item \textbf{Interface Inspection:} The Researcher will lead an inspection of the component interfaces (e.g., the data contract between the Computer Vision Model and the Game State Manager) defined in the Design Document.
    This ensures modularity and that the design supports FR and NFR related to communication and data exchange.
    \item \textbf{Structured Peer Review:} The design will be reviewed by all team members to check for feasibility, proper component decomposition, and adherence to design principles (e.g., modularity).
    \item \textbf{External Review:} Classmates will be asked to review the design, focusing on potential bottlenecks or single points of failure, particularly regarding real-time operation and $\AI{}$ performance constraints.
\end{itemize}


\subsection{Verification and Validation Plan Verification}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}


The VnV Plan itself is a critical project artifact that must be verified for feasibility, clarity, and completeness.

\begin{itemize}
    \item \textbf{Team Review:} The plan will be reviewed by all team members, ensuring that the defined VnV tasks are feasible within the academic timeframe and that the level of detail is sufficient for execution.
    \item \textbf{Feasibility Assessment:} The Team Leader will specifically check that the execution of the full VnV plan is realistic given the eight-month project timeline and the team’s current skill set.
    \item \textbf{External Feedback:} The plan's clarity and completeness will be verified by the course TA and classmates during informal review sessions.
\end{itemize}


\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

Implementation verification is focused on ensuring the source code correctly implements the design and fulfills the requirements.
This will use a combination of dynamic (automated tests) and non-dynamic (static analysis and inspection) techniques.

\begin{itemize}
    \item \textbf{Unit Testing and System Tests:} All functional and non-functional requirements will be verified through the comprehensive dynamic tests described in the \hyperref[sec:srs-system]{System Tests} and \hyperref[sec:unit-test-description]{Unit Test Description} sections of this document.
    \item \textbf{Static Analysis and Linters:} The team will enforce coding standards (PEP 8 for Python, Google Style Guide for JavaScript/React) using automated linters and static analyzers (SonarQube).
    \item \textbf{Code Walkthrough:} The final class presentation in CAS 741 (Final Demonstration, March 23) will serve as a high-level code walkthrough for a portion of the system, focusing on key components like the $\CV{}$ integration or $\AI{}$ decision-making process.
    \item \textbf{Peer Code Inspection:} All code changes will require a review and approval via GitHub Pull Requests.
    Team members will specifically check for adherence to coding standards, algorithm correctness, and proper error handling.
\end{itemize}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}


Automated VnV tools are central to maintaining code quality and continuous integration.

\begin{itemize}
    \item \textbf{Unit Testing Frameworks:} Python’s built-in \texttt{unittest} or \texttt{pytest} will be used for backend unit testing. Jest will be used for the frontend React components.
    \item \textbf{Continuous Integration (CI):} GitHub Actions will be implemented to run automated tests, linters, and build checks upon every push or pull request.
    \item \textbf{Static Analysis:} SonarQube will be used to analyze the codebase, identify code smells, potential bugs, and security vulnerabilities.
    \item \textbf{Code Coverage:} Tools like Coverage.py (for Python) will be integrated into the CI pipeline to report and track unit test coverage metrics, aiming for a minimum of 80\% coverage for core logic modules.
    \item \textbf{Linters:} \texttt{flake8} will enforce the PEP 8 standard for all Python backend code. ESLint will enforce the Google Style Guide for the JavaScript/React frontend.
\end{itemize}



\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}


Software validation ensures the final product meets the actual needs of the end-users (players) and stakeholders.

\begin{itemize}
    \item \textbf{Revision 0 Demonstration with Supervisor:} The Revision 0 Demonstration (Feb. 2) will be specifically used to validate that the core functionality and $\AI{}$ performance meet the project goals. Dr. Istvan David will provide critical feedback on the $\AI{}$'s strategic viability and adherence to the project scope.
    \item \textbf{User-Centric Validation (Usability Survey):} A partial usability survey will be conducted (using the final class presentation or an informal user testing session with competitive $\Catan{}$ players) to validate the $\NFR{}$ related to usability (NFR.S.2) and the clarity of the $\AI{}$ advice. Survey questions will be included in the Appendix.
    \item \textbf{External Data Validation:} The $\RL{}$ agent's performance will be validated longitudinally by running thousands of game simulations and comparing its win rate and strategic output against existing benchmark $\Catan{}$ opponents (e.g., baseline bots), and potentially against human gameplay data provided by competitive players. This output will be summarized in the \textbf{Performance Report} extra.
\end{itemize}


\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}


This section details the system-level tests for validating the functional and non-functional requirements of the RLCatan system.
The system tests are designed to be run once the integrated system is complete, using real-world or representative data to simulate actual use cases.
Traceability to specific requirements (e.g., $\hyperref[FR.S.1.1]{FR.S.1.1}$) is maintained throughout.



\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}


This subsection outlines test cases organized by the primary functional component responsible for the requirement.
These tests collectively cover all functional requirements detailed in Section S.2 of the SRS, including the core components: Computer Vision, RL Model, and Game State Management.
(add more references maybe)



\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}


\subsection{Tests for Functional Requirements}


This subsection details the system tests for all functional requirements ($\FR{}$) outlined in the SRS (Section S.2).
The tests are categorized by the major functional areas of the RLCatan system to ensure comprehensive coverage.
The Test IDs maintain clear traceability to their corresponding requirements.


\subsubsection{AI Model and Recommendation Logic}

This area focuses on verifying the core $\AI{}$ functionality, including strategy prediction ($\hyperref[FR.S.3.1]{FR.S.3.1}$), logical evaluation of moves ($\hyperref[FR.S.3.2]{FR.S.3.2}$), and integration with the $\RL{}$ Environment ($\hyperref[FR.S.2.5]{FR.S.2.5}$) and the official game rules ($\hyperref[FR.S.2.1]{FR.S.2.1}$).
These tests ensure the $\AI{}$ provides optimal, valid, and context-aware suggestions.

\textbf{Test Case: $\AI{}$ Move Validity and Strategy Prediction}

\begin{enumerate}

\item{T.FR.AI.2.1\\}

Control: Automatic

Initial State: A game state is loaded where Player A has exactly 1 Wood, 1 Brick, and 2 Roads left to build.
All adjacent spaces for a new road are occupied by other players' structures, leaving only one legal road placement location.
Player A has no other legal moves (cannot build a settlement, city, or development card).

Input: The structured $\GameState{}$ is sent to the $\AI{}$ model via the Game State Manager ($\hyperref[FR.S.2.5]{FR.S.2.5}$).

Output: The $\AI{}$'s move suggestion must be \texttt{BuildRoad} at the single remaining legal location.
No illegal moves (e.g., \texttt{BuildSettlement} or suggestions to trade without partners) should be returned.

Test Case Derivation: Verifies $\hyperref[FR.S.3.1]{FR.S.3.1}$ (Strategy Prediction) and $\hyperref[FR.S.3.2]{FR.S.3.2}$ (Evaluate potential moves) by enforcing a state with only one legal optimal move, thus checking adherence to rules ($\hyperref[FR.S.2.1]{FR.S.2.1}$) and logical constraints.

How test will be performed: An automated simulation within the $\RL{}$ Environment ($\hyperref[FR.S.2.1]{FR.S.2.1}$) is run 10 times from this exact pre-set state.
All 10 runs must result in the suggestion of the single legal road placement.

\item{T.FR.AI.2.2\\}

Control: Manual

Initial State: $\GameState{}$ where Player A is 1 Victory Point (VP) away from winning (e.g., 9 VP + Longest Road) and has enough resources to build a settlement, but not a city.
An open, unblocked node that grants the winning 10th VP is available.

Input: $\GameState{}$ where the immediate winning move is available.

Output: The $\AI{}$ must provide the suggestion to \texttt{BuildSettlement} at the winning node, with a high confidence score ($\hyperref[FR.S.3.4]{FR.S.3.4}$). The textual reasoning should explicitly state that this move secures the victory.

Test Case Derivation: Verifies $\hyperref[FR.S.3.1]{FR.S.3.1}$ (Strategy Prediction) by testing the $\AI{}$'s ability to identify and prioritize an immediate game-winning condition over resource maximization.

How test will be performed: Load the state and manually check the UI display ($\hyperref[FR.S.4.4]{FR.S.4.4}$) to ensure the winning move is highlighted and prioritized over all other options.

\item{T.FR.AI.2.3\\}

Control: Automatic

Initial State: $\GameState{}$ loaded for a 4-player game where the current player (Player A) has 6 resources and rolls a 7, triggering the Robber phase.

Input: The structured $\GameState{}$ is fed to the $\AI{}$ model.

Output: The $\AI{}$ must correctly calculate that Player A needs to discard 3 cards (half of their resources, rounded down), and must suggest a move (placing the Robber, stealing a card) that is fully legal, verifying the rule simulation ($\hyperref[FR.S.2.1]{FR.S.2.1}$).

Test Case Derivation: Checks a critical game transition rule (Robber movement/discarding) against the simulated environment and $\AI{}$ logic, ensuring adherence to the core ruleset.

How test will be performed: Run 10 simulations where the Robber is rolled.
Verify the system logic correctly applies the discarding rule and the $\AI{}$ suggests a valid Robber placement move.
\end{enumerate}

\subsubsection{User Interface and Data Management}

This area verifies that the system correctly manages, displays, and archives game data, supporting both real-time user interaction ($\hyperref[FR.S.4.1]{FR.S.4.1}$ to $\hyperref[FR.S.4.6]{FR.S.4.6}$) and persistent data storage ($\hyperref[FR.S.5.1]{FR.S.5.1}$ to $\hyperref[FR.S.5.5]{FR.S.5.5}$). This ensures data integrity and user experience.

\textbf{Test Case: Real-time UI Update and Database Integrity}

\begin{enumerate}

\item{T.FR.UI.3.1\\}

Control: Manual/Automatic

Initial State: Game has just started.
Player A has 2 settlements and 2 roads. The UI displays Player A's resource inventory as empty.

Input: Player A performs the action \texttt{BuildRoad} at node X, which is processed by the Game State Manager ($\hyperref[FR.S.7.4]{FR.S.7.4}$) and updated in the $\GameState{}$ Database ($\hyperref[FR.S.5.1]{FR.S.5.1}$) using the Image Queue ($\hyperref[FR.S.6.1]{FR.S.6.1}$).

Output:
    a) The UI must immediately reflect the change, showing the new road on the board visualization ($\hyperref[FR.S.4.1]{FR.S.4.1}$) and Player A's resource inventory accurately reduced ($\hyperref[FR.S.4.2]{FR.S.4.2}$).
    b) The $\GameState{}$ Database entry for this turn must be complete, correctly logging the pre-move state, the move performed, and the resulting post-move state ($\hyperref[FR.S.5.2]{FR.S.5.2}$).

Test Case Derivation: Verifies the real-time data flow requirements ($\hyperref[FR.S.6.1]{FR.S.6.1}$) and the system's ability to automatically and accurately update all components after a player action ($\hyperref[FR.S.7.4]{FR.S.7.4}$).

How test will be performed: Perform 5 consecutive build actions and measure the time for the UI to update.
After the sequence, query the $\GameState{}$ Database to confirm that 5 distinct, consistent, and sequentially correct game states were logged ($\hyperref[FR.S.5.1]{FR.S.5.1}$).

\item{T.FR.UI.3.2\\}

Control: Automatic

Initial State: Game is 50 turns long and complete.
The game state database contains the full history.

Input: Request historical game data ($\hyperref[FR.S.5.5]{FR.S.5.5}$) for the completed game, specifically requesting all dice rolls and all development card purchases.

Output: The system successfully returns the complete list of 50 turn records, including the dice roll for each turn, and a timestamped record of every development card purchased, demonstrating access to historical data ($\hyperref[FR.S.7.3]{FR.S.7.3}$).

Test Case Derivation: Verifies the requirements for historical logging and structured access to past game data ($\hyperref[FR.S.5.5]{FR.S.5.5}$).

How test will be performed: An automated script runs 10 full simulated games, each over 40 turns.
After each game, the script attempts to query the database for the complete turn log.
The test passes only if all 10 logs are complete, consistent, and accurately reflect the game rules.

\end{enumerate}

\subsubsection{Computer Vision and Game State Capture}

This area focuses on verifying the accuracy and reliability of the Computer Vision (CV) Model ($\FR{}$.S.1.x) and the Game State Manager's ($\FR{}$.S.7.x) ability to convert visual input into a consistent digital representation ($\GameState{}$).
The tests ensure the system can accurately detect board elements ($\hyperref[FR.S.1.1]{FR.S.1.1}$), translate them into structured data ($\hyperref[FR.S.1.2]{FR.S.1.2}$), and maintain synchronization ($\hyperref[FR.S.7.1]{FR.S.7.1}$).

\textbf{Test Case: Initial Board Setup Recognition}

\begin{enumerate}

\item{T.FR.CV.1.1\\}

Control: Manual

Initial State: A standard, legal Catan board is set up for the initial placement phase.
Resource tiles, number tokens, and the Robber are placed according to the rules. No settlements or roads have been placed.

Input: A captured image (or video frame) of the physical board.

Output: A structured $\GameState{}$ data object where the board topology (tile types and locations, number tokens, Robber position) is correctly mapped to the digital twin.
The asset counts for all players (settlements: 5, roads: 15) must be correct, verifying $\hyperref[FR.S.7.2]{FR.S.7.2}$.

Test Case Derivation: Directly verifies $\hyperref[FR.S.1.2]{FR.S.1.2}$ (Feature-to-State Translation) and $\hyperref[FR.S.7.1]{FR.S.7.1}$ (State Synchronization) using a known baseline state.

How test will be performed: A set of 5 different, legally generated board setups will be used.
For each setup, the system is fed the image, and the output $\GameState{}$ dictionary is inspected to ensure a 100\% match with the actual setup.

\item{T.FR.CV.1.2\\}

Control: Manual

Initial State: A game is mid-play.
Player A has 2 settlements, 1 city, 8 roads. The camera is slightly misaligned, causing a minor occlusion of one edge.

Input: A captured image of the board with the occlusion.

Output: The system successfully detects the presence of Player A's 2 settlements, 1 city, and 8 roads, correctly applying logical constraints to infer the state despite the occlusion.
The system provides a diagnostic confidence metric ($\hyperref[FR.S.1.5]{FR.S.1.5}$) for the occluded area.

Test Case Derivation: Verifies $\hyperref[FR.S.1.3]{FR.S.1.3}$ (Error Detection and Correction) and $\hyperref[FR.S.1.5]{FR.S.1.5}$ (Diagnostic Feedback) under sub-optimal real-world conditions.

How test will be performed: Artificially introduce a small, known occlusion (e.g., a hand partially covering an intersection) in a mid-game state.
Check that the digital state is correctly reconstructed and that the confidence metric correctly flags the occluded element as low-confidence/corrected.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

The table below documents the explicit traceability between the defined system test cases ($\mathit{T.ID}$) and the specific requirements ($\mathit{Req.ID}$) they verify. This ensures that every functional and critical non-functional requirement is covered with appropriate redundancy, building confidence in the product if all tests pass.

\begin{table}[h!]
    \centering
    \caption{System Test Case Traceability Matrix (Partial)}
    \label{tab:system-test-traceability}
    \begin{tabularx}{\textwidth}{|c|X|}
    \hline
    \textbf{Test Case ID} & \textbf{Requirements Verified (Subset)} \\
    \hline
    T.FR.AI.2.1 & $\hyperref[FR.S.3.1]{FR.S.3.1}$, $\hyperref[FR.S.3.2]{FR.S.3.2}$, $\hyperref[FR.S.2.1]{FR.S.2.1}$, $\hyperref[FR.S.2.5]{FR.S.2.5}$, $\hyperref[FR.Sa.3]{FR.Sa.3}$ \\
    \hline
    T.FR.AI.2.2 & $\hyperref[FR.S.3.1]{FR.S.3.1}$, $\hyperref[FR.S.3.4]{FR.S.3.4}$ \\
    \hline
    T.FR.AI.2.3 & $\hyperref[FR.S.2.1]{FR.S.2.1}$ \\
    \hline
    T.FR.UI.3.1 & $\hyperref[FR.S.4.1]{FR.S.4.1}$, $\hyperref[FR.S.4.2]{FR.S.4.2}$, $\hyperref[FR.S.5.1]{FR.S.5.1}$, $\hyperref[FR.S.5.2]{FR.S.5.2}$, $\hyperref[FR.S.6.1]{FR.S.6.1}$, $\hyperref[FR.S.7.4]{FR.S.7.4}$, $\hyperref[FR.S.7.2]{FR.S.7.2}$ \\
    \hline
    T.FR.UI.3.2 & $\hyperref[FR.S.5.5]{FR.S.5.5}$, $\hyperref[FR.S.7.3]{FR.S.7.3}$ \\
    \hline
    T.FR.CV.1.1 & $\hyperref[FR.S.1.2]{FR.S.1.2}$, $\hyperref[FR.S.7.1]{FR.S.7.1}$, $\hyperref[FR.S.7.2]{FR.S.7.2}$ \\
    \hline
    T.FR.CV.1.2 & $\hyperref[FR.S.1.3]{FR.S.1.3}$, $\hyperref[FR.S.1.5]{FR.S.1.5}$ \\
    \end{tabularx}
\end{table}

\section{Tests for Nonfunctional Requirements}

This section outlines the system-level tests for validating the nonfunctional requirements (NFRs) defined in Section~S.2.8 of the SRS. These include scalability, usability, maintainability, installability, data integrity, and availability. 

Each subsection corresponds to a major nonfunctional quality attribute of the RLCatan system. Some tests produce summary statistics or qualitative evaluations (e.g., graphs or surveys) rather than strict pass/fail outcomes.  

Accuracy-related nonfunctional qualities are validated implicitly through the functional tests defined in Section~4.2 (T.FR.AI.2.1--2.3 and T.FR.CV.1.1--1.2), which record relative error and correctness metrics.

\subsection{Performance and Scalability}

This area ensures the RLCatan system maintains acceptable responsiveness and throughput when supporting multiple concurrent games.

\textbf{Test Case: Scalability and Response Time Evaluation}

\begin{enumerate}
\item \textbf{T.NFR.P.1} \\
\textbf{Control:} Automatic \\
\textbf{Initial State:} The RLCatan server is running the complete software stack (AI, CV, Database, UI). \\
\textbf{Input/Condition:} Simulate 1, 3, and 5 concurrent games using the RL environment and mock UI clients. \\
\textbf{Output/Result:} Average response time per game, CPU load, and memory usage are measured and summarized in a graph. \\
\textbf{Test Case Derivation:} Verifies NFR.S.1 (Scalability) by demonstrating that response time increases sub-linearly with load. \\
\textbf{How test will be performed:} Automated load-testing scripts measure latency between GameState updates and AI recommendations. Results are summarized in a table of performance metrics versus number of sessions.
\end{enumerate}

\subsection{Usability}

This area focuses on confirming that the interface is intuitive and accessible to users with minimal training. It also includes accessibility checks for users with visual impairments.

\textbf{Test Case: Usability Survey and User Feedback}

\begin{enumerate}
\item \textbf{T.NFR.U.1} \\
\textbf{Control:} Manual \\
\textbf{Initial State:} Final prototype deployed in a browser. \\
\textbf{Input/Condition:} 5--10 participants familiar with Catan complete a guided session and fill out the usability survey. \\
\textbf{Output/Result:} Average rating $\geq 4/5$ for clarity, ease of navigation, and understanding of AI feedback. \\
\textbf{Test Case Derivation:} Verifies NFR.S.2 (Usability) through direct user evaluation. \\
\textbf{How test will be performed:} Conducted during the final demo; results aggregated and visualized as a bar chart.
\end{enumerate}

\textbf{Test Case: Accessibility and Visual Clarity}

\begin{enumerate}
\item \textbf{T.NFR.U.2} \\
\textbf{Control: Manual} \\
\textbf{Initial State:} RLCatan UI loaded in browser. \\
\textbf{Input/Condition:} Apply color-blindness filters and vary display scaling. \\
\textbf{Output/Result:} All critical UI elements remain legible; contrast ratio $\geq 4.5:1$. \\
\textbf{Test Case Derivation:} Verifies accessibility and clarity under varied visual conditions. \\
\textbf{How test will be performed:} Tested using Chrome DevTools accessibility simulator; screenshots recorded for documentation.
\end{enumerate}

\subsection{Maintainability}

This area ensures the modular design and structure of the codebase support efficient updates and debugging.

\textbf{Test Case: Code Quality and Modular Design Inspection}

\begin{enumerate}
\item \textbf{T.NFR.M.1} \\
\textbf{Control:} Static \\
\textbf{Initial State:} Complete source code committed to GitHub. \\
\textbf{Input/Condition:} Run \texttt{SonarQube} and \texttt{flake8} to generate maintainability and style reports. \\
\textbf{Output/Result:} No high-severity issues; modules follow consistent structure and clear separation of concerns. \\
\textbf{Test Case Derivation:} Verifies NFR.S.3 (Maintainability) through code inspection and static analysis. \\
\textbf{How test will be performed:} Peer inspection led by group members; issues logged in GitHub for resolution.
\end{enumerate}

\subsection{Installability}

This area validates that RLCatan installs and operates correctly across all supported operating systems and browsers.

\textbf{Test Case: Cross-Platform Installation Validation}

\begin{enumerate}
\item \textbf{T.NFR.I.1} \\
\textbf{Control:} Manual \\
\textbf{Initial State:} Packaged release available. \\
\textbf{Input/Condition:} Install and execute RLCatan on Windows~11 and macOS using Chrome and Firefox browsers. \\
\textbf{Output/Result:} Application installs and runs successfully on all tested platforms. \\
\textbf{Test Case Derivation:} Verifies NFR.S.4 (Installability) by ensuring cross-platform compatibility. \\
\textbf{How test will be performed:} Each team member installs on a unique OS following a checklist; issues recorded and documented.
\end{enumerate}

\subsection{Data Integrity}

This area ensures that the Game State Database maintains consistent, valid, and accurate data under all conditions, including normal operation, concurrent updates, and unexpected failures. This includes validation of incoming updates and transactional consistency to prevent corruption.

\textbf{Test Case: Transaction Integrity and Recovery}

\begin{enumerate}
\item \textbf{T.NFR.D.1} \\
\textbf{Control:} Automatic \\
\textbf{Initial State:} Active game session connected to the database. \\
\textbf{Input/Condition:} Execute 10 consecutive state updates with an artificial network interruption during one update. \\
\textbf{Output/Result:} Database remains consistent; interrupted transaction is rolled back cleanly; no other game states are affected. \\
\textbf{Test Case Derivation:} Verifies transactional consistency portion of NFR.S.5 under failure conditions. \\
\textbf{How test will be performed:} Automated integration test simulates network failures and verifies database consistency after each run.
\end{enumerate}

\textbf{Test Case: Data Validation on Updates}

\begin{enumerate}
\item \textbf{T.NFR.D.2} \\
\textbf{Control:} Automatic \\
\textbf{Initial State:} Active game session connected to the database. \\
\textbf{Input/Condition:} Attempt to submit invalid game state updates, such as negative resource counts, illegal moves, or missing required fields. \\
\textbf{Output/Result:} Invalid updates are recongized and rejected \\
\textbf{Test Case Derivation:} Verifies validation portion of NFR.S.5, preventing data corruption from malformed inputs. \\
\textbf{How test will be performed:} Automated tests inject invalid game state objects and verifies rejection.\\
\end{enumerate}

\textbf{Test Case: Concurrent Update Handling}

\begin{enumerate}
\item \textbf{T.NFR.D.3} \\
\textbf{Control:} Automatic \\
\textbf{Initial State:} Two or more simultaneous game sessions active. \\
\textbf{Input/Condition:} Simulate concurrent updates to overlapping resources (e.g., two players updating the same tile). \\
\textbf{Output/Result:} Only one valid transaction is committed; conflicts are detected and resolved; no data corruption occurs. \\
\textbf{Test Case Derivation:} Verifies database integrity under concurrent operations, completing NFR.S.5 coverage. \\
\textbf{How test will be performed:} Automated integration tests run multiple simulated clients updating shared resources; system logs and database state are verified for correctness.
\end{enumerate}


\subsection{Availability}

This area verifies that the system automatically recovers from failures within one minute, as required in the SRS.

\textbf{Test Case: System Failure and Auto-Recovery}

\begin{enumerate}
\item \textbf{T.NFR.AV.1} \\
\textbf{Control:} Manual \\
\textbf{Initial State:} System running with active game session. \\
\textbf{Input/Condition:} Force a server crash or database disconnect mid-turn. \\
\textbf{Output/Result:} System restarts and restores previous game state within 60~seconds. \\
\textbf{Test Case Derivation:} Verifies NFR.S.6 (Availability) by measuring recovery time following induced failure. \\
\textbf{How test will be performed:} Use shell script to crash the process, record downtime using system logs, verify recovery from last snapshot.
\end{enumerate}

\subsection{Accuracy and Correctness (Referenced)}

Accuracy-related NFRs are verified by functional tests defined in Section~4.2:
\begin{itemize}
    \item T.FR.AI.2.1--2.3 (AI Model and Recommendation Logic)
    \item T.FR.CV.1.1--1.2 (Computer Vision and Game State Capture)
\end{itemize}
These tests report relative error between predicted and expected outcomes. No separate nonfunctional test cases are required.

\subsection{Traceability Between Test Cases and Requirements }

\begin{table}[H]
  \centering
  \caption{Traceability Between Nonfunctional Test Cases and Requirements}
  \label{tab:system-test-traceability}
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Test Case ID} & \textbf{Requirement(s) Verified} \\ \hline
    T.NFR.P.1   & NFR.S.1 -- Scalability \\ \hline
    T.NFR.U.1   & NFR.S.2 -- Usability \\ \hline
    T.NFR.U.2   & NFR.S.2 -- Accessibility \\ \hline
    T.NFR.M.1   & NFR.S.3 -- Maintainability \\ \hline
    T.NFR.I.1   & NFR.S.4 -- Installability \\ \hline
    T.NFR.D.1   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.D.2   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.D.3   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.AV.1  & NFR.S.6 -- Availability \\ \hline
  \end{tabular}
\end{table}


\section{Unit Test Description}
This section is to be completed after the MIS (detailed design document) has been completed.

\begin{comment}
\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
\end{comment}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\subsection*{Jake Read}
\begin{enumerate}
    \item This deliverable went fairly smoothly, mostly because we already had a clear set of requirements from the SRS to base the tests on.
    Unlike the previous deliverables, we split into two subteams.
    Matthew and Rebecca worked primarily on this doc, while Sunny and I focused on getting started with the PoC, and then checked in with the VnV team to help review.
    I feel like this made the process more efficient, and less people working directly on one document meant fewer consistency issues.
    \item Personally, I didn't have any major pain points with the VnV plan, but as I said before, I was mainly focused on the PoC and simply reviewed and helped organize VnV progress.
    I think Rebecca and Matthew did a great job, so the review process was quite smooth.
    It's possible they had some pain points I'm not aware of, but from my discussions with them this doc was more straightforward than the SRS.
    \item For VnV, I think the team will need to acquire more knowledge about automated testing frameworks, particularly for Python and JavaScript.
    I'm not sure how experienced the others are in these areas, but when it comes to testing almost all my experience is in Java, so I think this will be a learning opportunity for me.
    We'll also need to familiarize ourselves with static analysis tools like SonarQube, as well as CI/CD pipelines using GitHub Actions.
    I can focus on SonarQube, since I'm interested in using it for future projects.
    \item For learning about automated testing frameworks, we could either take online courses/tutorials or read official documentation and implement small projects to practice.
    I also previously used SonarQube briefly back in second year, so I could revisit that experience and supplement it with online resources.
    Of these options, I'll start with some online tutorials, and then reference what I did with some of my old work.
\end{enumerate}

\subsection*{Rebecca DiFilippo}
\begin{enumerate}
    \item Writing this deliverable went fairly smoothly, mainly because we had a solid set of 
    requirements from the SRS to guide the tests. Splitting into two subteams worked really well
     this time. Matthew and I focused on drafting the V\&V document, while Sunny and Jake worked 
     on the PoC and then helped review our work. I think this setup made things more organized and
      helped avoid overlapping edits and consistency issues. It also allowed us to make progress
      on the POC plan.
    
    \item I didn’t run into major pain points, though coordinating between the two subteams
     sometimes required a bit of back-and-forth to make sure everyone was aligned. Overall, Matthew
      and I were able to keep the doc consistent, and Jake and Sunny’s feedback was really helpful
       for catching small issues early. The process felt much smoother than working on the SRS. 
    
    \item For V\&V, the team will need to build more experience with automated testing frameworks.We also we’ll need to
       get comfortable with static analysis tools like SonarQube and CI/CD pipelines through GitHub
        Actions. I’ll focus on automated testing in Python since that’s an area I want to strengthen
         for this project and future work which includes flake8 style checks.
    
    \item To build these skills, we can look for online  tutorials and courses, as well as consult
      with other people who have experience in these areas. I’ll start with online tutorials for
       Python testing frameworks and flake8, as well as look for example projects that implement
        these tools effectively.
\end{enumerate}

\end{document}