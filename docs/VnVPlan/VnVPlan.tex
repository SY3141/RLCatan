\documentclass[12pt, titlepage]{article}

\usepackage{comment}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amstext}
\usepackage{zed-csp}
\usepackage{pdflscape}
\usepackage{caption}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{placeins}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

% ===== External links (used only inside Glossary) =====
\newcommand{\CatanExt}{\href{https://en.wikipedia.org/wiki/Catan}{Catan}}
\newcommand{\AIExt}{\href{https://en.wikipedia.org/wiki/Artificial_intelligence}{AI}}
\newcommand{\RLExt}{\href{https://www.ibm.com/think/topics/reinforcement-learning}{RL}}
\newcommand{\DigitalTwinExt}{\href{https://en.wikipedia.org/wiki/Digital_twin}{Digital Twin}}
\newcommand{\CVExt}{\href{https://www.ibm.com/think/topics/computer-vision}{CV}}
\newcommand{\LLMExt}{\href{https://www.cloudflare.com/learning/ai/what-is-large-language-model/}{LLM}}
\newcommand{\GameStateExt}{\href{https://milvus.io/ai-quick-reference/what-is-a-state-in-rl}{Game State}}

\newcommand{\NFRExt}{\href{https://en.wikipedia.org/wiki/Non-functional_requirement}{NFR}}
\newcommand{\FRExt}{\href{https://en.wikipedia.org/wiki/Functional_requirement}{FR}}


\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
10/19/2025 & Draft & Initial VnV plan without Unit Testing\\
... & ... & ...\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables


\listoffigures


\newpage



% ===== Internal glossary link targets =====
\hypertarget{glossary-catan}{}
\hypertarget{glossary-ai}{}
\hypertarget{glossary-rl}{}
\hypertarget{glossary-dt}{}
\hypertarget{glossary-cv}{}
\hypertarget{glossary-llm}{}
\hypertarget{glossary-gamestate}{}
\hypertarget{glossary-nfr}{}
\hypertarget{glossary-fr}{}

% ===== Internal references (used throughout document) =====
\newcommand{\Catan}{\hyperlink{glossary-catan}{Catan}}
\newcommand{\AI}{\hyperlink{glossary-ai}{AI}}
\newcommand{\RL}{\hyperlink{glossary-rl}{RL}}
\newcommand{\DigitalTwin}{\hyperlink{glossary-dt}{Digital Twin}}
\newcommand{\CV}{\hyperlink{glossary-cv}{CV}}
\newcommand{\LLM}{\hyperlink{glossary-llm}{LLM}}
\newcommand{\GameState}{\hyperlink{glossary-gamestate}{Game State}}
\newcommand{\NFR}{\hyperlink{glossary-nfr}{NFR}}
\newcommand{\FR}{\hyperlink{glossary-fr}{FR}}

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\


The following glossary headers are hyperlinked to their online definitions for further reading.
\begin{itemize}
    \item \hypertarget{glossary-catan}{\CatanExt{}} – A strategy board game called \textit{Settlers of Catan} where players collect resources, build roads/settlements, and trade to earn points.[1]
    \item \hypertarget{glossary-ai}{\AIExt{}} – Field of computer science and engineering that focuses on creating systems capable of performing tasks that usually require human intelligence.[2]
    \item \hypertarget{glossary-rl}{\RLExt{}} – A type of machine learning where an agent, known as Reinforcement Learning, learns by interacting with an environment and receiving rewards or penalties for its actions.[3]
    \item \hypertarget{glossary-dt}{\DigitalTwinExt{}} – A digital system that mirrors a physical one. In this project, it refers to a digital representation of the physical \emph{Catan} board, updated in real time.[4]
    \item \hypertarget{glossary-cv}{\CVExt{}} – An area of AI that trains computers to interpret and understand visual information, such as the physical \emph{Catan} board.[5]
    \item \hypertarget{glossary-llm}{\LLMExt{}} – A machine learning model trained on large amounts of text data to generate and understand language.[6]
    \item \hypertarget{glossary-gamestate}{\GameStateExt{}} – The current configuration of the game, including player resources, board layout, and dice rolls.[7]
    \item \hypertarget{glossary-fr}{\FRExt{}} – A requirement that defines a function of the system; it specifies what the system must do.[8]
    \item \hypertarget{glossary-nfr}{\NFRExt{}} – A requirement that specifies a quality attribute of the system; it defines how the system must be.[9]
\end{itemize}

\newpage

\pagenumbering{arabic}

This document presents the Verification and Validation (V\&V) plan for RLCatan. The plan will be updated and refined after the MIS (detailed design document) is completed.


\section{General Information}

\subsection{Summary}


  The software being tested is RLCatan,
 an $\AI{}$  system that learns to play the board game $\Catan{}$
 competitively through reinforcement learning. 
 It includes a learning agent trained in a simulated
  environment, a computer vision module for detecting 
  and interpreting the physical game board, and a
  interface that displays the game state and $\AI{}$-generated
  move suggestions to users. The general purpose of RLCatan
  is to provide optimal move suggestions to human players
  based on real-time analysis of the game state.

\subsection{Objectives}


The objective is to build confidence 
in the correctness, reliability, and functional performance of the RLCatan system.
 Verification will focus on ensuring that the reinforcement learning agent generates 
 valid moves, that the computer vision module correctly interprets game states, and 
 that data exchange between system components is accurate and consistent.

\medskip

 Out of scope are extensive usability studies and large-scale performance benchmarking,
 as these exceed the project’s time and resource constraints. Additionally, external
  dependencies such as Catanatron, OpenCV, and YOLOv9 are assumed to be verified by
   their developers, and will only be tested for correct integration within RLCatan.

\subsection{Challenge Level and Extras}



The challenge level for this project is Advanced,
 as outlined in the problem statement and confirmed 
 with the course instructor. This project
  reflects the complexity of developing a 
  reinforcement learning agent capable of
   mastering \textit{Settlers of $\Catan{}$},
    integrating computer vision for real-time game state 
    analysis, and ensuring seamless interaction between
     system components.

\medskip
The approved extras for this project include:
\begin{itemize}
    \item User Instructional Video: A walkthrough video that explains how to set up, run, and interact with the software, helping users understand its functionality and workflow.
    \item Performance Report: A comprehensive report summarizing the $\AI{}$'s performance metrics, learning progression, and evaluation results based on various test scenarios.
\end{itemize}

\subsection{Relevant Documentation}

The following documents provide essential background for the VnV plan:

\begin{enumerate}
  \item \textbf{Software Requirements Specification (\citet{SRS}):} This document outlines the functional and non-functional requirements for RLCatan, serving as the primary reference for the verification and validation plan.
  \item \textbf{Hazard Analysis (\citet{HA}):} This document identifies potential system-level hazards, their causes, and their effects.
  The functional and non-functional safety requirements derived in the Hazard Analysis are the basis for many of the critical system tests in this VnV plan.
\end{enumerate}


\section{Plan}

This section outlines the strategy for Verification and Validation (VnV) across all phases of the RLCatan project lifecycle.
The plan details the roles and responsibilities of the VnV team, and specifies structured approaches for verifying the Software Requirements Specification (SRS), Design, and Implementation, as well as the strategy for Software Validation.
This systematic approach is designed to increase confidence in the correctness, functional performance, and reliability of the final system.
(insert visual roadmap)


\subsection{Verification and Validation Team}




The VnV efforts will be a collaborative team effort, with specific responsibilities assigned based on existing project roles to ensure systematic coverage and accountability.

\begin{table}[h!]
  \centering
  \caption{Verification and Validation Team Roles}
  \label{tab:vnv-team-roles}
  \begin{tabularx}{\textwidth}{|p{2cm}|p{2cm}|X|}
  \hline
  \textbf{Team Member} & \textbf{Project Role} & \textbf{VnV Responsibilities} \\ \hline
  Jake Read & Team Leader & Coordinating all VnV activities, performing final review of all test cases, and leading the code walkthrough for the supervisor. \\ \hline
  Rebecca Di Filippo & Notetaker & Documenting VnV meeting minutes, maintaining traceability tables, and preparing the Usability Survey for validation. \\ \hline
  Sunny Yao & IT/DevOps & Implementing and maintaining CI/CD pipelines, monitoring automated test coverage metrics, and troubleshooting technical VnV tool issues. \\ \hline
  Matthew Cheung & Researcher & Researching and applying non-dynamic testing techniques (e.g., formal code inspection checklists) and collecting external data for software validation. \\ \hline
  Dr. Istvan David & Project Supervisor & Providing technical oversight, expert consultation on $\RL{}$ model correctness, and participating in the Revision 0 demonstration for requirements validation. \\ \hline
  \end{tabularx}
\end{table}

\FloatBarrier


\subsection{SRS Verification}



The SRS will be verified against the project’s objectives to ensure all requirements are unambiguous, correct, feasible, and complete (Quality System Tests).

\begin{itemize}
    \item \textbf{Structured Review with Supervisor:} A dedicated meeting will be scheduled with Dr. Istvan David, the Project Supervisor.
    The team will present the consolidated functional, non-functional, and safety requirements, specifically focusing on the formalizations (e.g., Z-Notation) and the key constraints (e.g., $\leq 5$ second response time for advice).
    Specific questions will be prepared beforehand to validate the technical accuracy of the $\RL{}$ requirements (e.g., reward function structure).
    \item \textbf{Peer Review and Inspection:} Classmates and primary reviewers will be engaged to perform an ad-hoc review of the SRS, targeting clarity and consistency.
    \item \textbf{Issue Tracker for Traceability:} All feedback and identified issues will be logged on the GitHub Issues board, assigned to a team member, and traced back to the specific requirement for resolution.
    \item \textbf{Checklist-Based Inspection:} The team will employ a custom checklist (developed by the Researcher) focusing on the characteristics of a high-quality requirement (unambiguous, verifiable, complete) to systematically inspect the SRS.
\end{itemize}



\subsection{Design Verification}


Design verification will occur after the Design Document is completed (Milestone: Nov. 10) to ensure the architectural design meets all requirements and is ready for implementation.

\begin{itemize}
    \item \textbf{Interface Inspection:} The Researcher will lead an inspection of the component interfaces (e.g., the data contract between the Computer Vision Model and the Game State Manager) defined in the Design Document.
    This ensures modularity and that the design supports FR and NFR related to communication and data exchange.
    \item \textbf{Structured Peer Review:} The design will be reviewed by all team members to check for feasibility, proper component decomposition, and adherence to design principles (e.g., modularity).
    \item \textbf{External Review:} Classmates will be asked to review the design, focusing on potential bottlenecks or single points of failure, particularly regarding real-time operation and $\AI{}$ performance constraints.
\end{itemize}


\subsection{Verification and Validation Plan Verification}



The VnV Plan itself is a critical project artifact that must be verified for feasibility, clarity, and completeness.

\begin{itemize}
    \item \textbf{Team Review:} The plan will be reviewed by all team members, ensuring that the defined VnV tasks are feasible within the academic timeframe and that the level of detail is sufficient for execution.
    \item \textbf{Feasibility Assessment:} The Team Leader will specifically check that the execution of the full VnV plan is realistic given the eight-month project timeline and the team’s current skill set.
    \item \textbf{External Feedback:} The plan's clarity and completeness will be verified by the course TA and classmates during informal review sessions.
\end{itemize}


\subsection{Implementation Verification}


Implementation verification is focused on ensuring the source code correctly implements the design and fulfills the requirements.
This will use a combination of dynamic (automated tests) and non-dynamic (static analysis and inspection) techniques.

\begin{itemize}
    \item \textbf{Unit Testing and System Tests:} All functional and non-functional requirements will be verified through the comprehensive dynamic tests described in the \hyperref[sec:srs-system]{System Tests} and \hyperref[sec:unit-test-description]{Unit Test Description} sections of this document.
    \item \textbf{Static Analysis and Linters:} The team will enforce coding standards (PEP 8 for Python, Google Style Guide for JavaScript/React) using automated linters and static analyzers (SonarQube).
    \item \textbf{Peer Code Inspection:} All code changes will require a review and approval via GitHub Pull Requests.
    Team members will specifically check for adherence to coding standards, algorithm correctness, and proper error handling.
\end{itemize}

\subsection{Automated Testing and Verification Tools}


Automated VnV tools are central to maintaining code quality and continuous integration.

\begin{itemize}
    \item \textbf{Unit Testing Frameworks:} Python’s built-in \texttt{unittest} or \texttt{pytest} will be used for backend unit testing. Jest will be used for the frontend React components.
    \item \textbf{Continuous Integration (CI):} GitHub Actions will be implemented to run automated tests, linters, and build checks upon every push or pull request.
    \item \textbf{Static Analysis:} SonarQube will be used to analyze the codebase, identify code smells, potential bugs, and security vulnerabilities.
    \item \textbf{Code Coverage:} Tools like Coverage.py (for Python) will be integrated into the CI pipeline to report and track unit test coverage metrics, aiming for a minimum of 80\% coverage for core logic modules.
    \item \textbf{Linters:} \texttt{flake8} will enforce the PEP 8 standard for all Python backend code. ESLint will enforce the Google Style Guide for the JavaScript/React frontend.
\end{itemize}



\subsection{Software Validation}



Software validation ensures the final product meets the actual needs of the end-users (players) and stakeholders.

\begin{itemize}
    \item \textbf{Revision 0 Demonstration with Supervisor:} The Revision 0 Demonstration (Feb. 2) will be specifically used to validate that the core functionality and $\AI{}$ performance meet the project goals. Dr. Istvan David will provide critical feedback on the $\AI{}$'s strategic viability and adherence to the project scope.
    \item \textbf{User-Centric Validation (Usability Survey):} A partial usability survey will be conducted (using the final class presentation or an informal user testing session with competitive $\Catan{}$ players) to validate the $\NFR{}$ related to usability (NFR.S.2) and the clarity of the $\AI{}$ advice. Survey questions will be included in the Appendix.
    \item \textbf{External Data Validation:} The $\RL{}$ agent's performance will be validated longitudinally by running thousands of game simulations and comparing its win rate and strategic output against existing benchmark $\Catan{}$ opponents (e.g., baseline bots), and potentially against human gameplay data provided by competitive players. This output will be summarized in the \textbf{Performance Report} extra.
\end{itemize}


\section{System Tests}


This section details the system-level tests for validating the functional and non-functional requirements of the RLCatan system.
The system tests are designed to be run once the integrated system is complete, using real-world or representative data to simulate actual use cases.
Traceability to specific requirements (e.g., $\hyperref[FR.S.1.1]{FR.S.1.1}$) is maintained throughout.



\subsection{Tests for Functional Requirements}


This subsection outlines test cases organized by the primary functional component responsible for the requirement.
These tests collectively cover all functional requirements detailed in Section S.2 of the SRS, including the core components: Computer Vision, $\RL{}$ Model, and Game State Management.
(add more references maybe)



\subsection{Tests for Functional Requirements}

This subsection details the system tests for all functional requirements ($\FR{}$) outlined in the SRS (Section S.2).
The tests are categorized by the major functional areas of the RLCatan system to ensure comprehensive coverage.
The Test IDs maintain clear traceability to their corresponding requirements.

\subsubsection{AI Model and Recommendation Logic}

This area focuses on verifying the core $\AI{}$ functionality, including strategy prediction ($\hyperref[FR.S.3.1]{FR.S.3.1}$), logical evaluation of moves ($\hyperref[FR.S.3.2]{FR.S.3.2}$), and integration with the $\RL{}$ Environment ($\hyperref[FR.S.2.5]{FR.S.2.5}$) and the official game rules ($\hyperref[FR.S.2.1]{FR.S.2.1}$). These tests ensure the $\AI{}$ provides optimal, valid, and context-aware suggestions.

\textbf{Test Case: $\AI{}$ Move Validity and Strategy Prediction}

\begin{enumerate}

\item{T.FR.AI.1.1\\}
\textbf{Control:} Automatic

\textbf{Initial State:} A game state is loaded where Player A has exactly 1 Wood, 1 Brick, and 2 Roads left to build. All adjacent spaces for a new road are occupied by other players' structures, leaving only one legal road placement location. Player A has no other legal moves (cannot build a settlement, city, or development card).

\textbf{Input:} The structured $\GameState{}$ is sent to the $\AI{}$ model via the Game State Manager ($\hyperref[FR.S.2.5]{FR.S.2.5}$).

\textbf{Output:} The $\AI{}$'s move suggestion must be \texttt{BuildRoad} at the single remaining legal location. No illegal moves (e.g., \texttt{BuildSettlement} or suggestions to trade without partners) should be returned.

\textbf{Test Case Derivation:} Verifies $\hyperref[FR.S.3.1]{FR.S.3.1}$ (Strategy Prediction) and $\hyperref[FR.S.3.2]{FR.S.3.2}$ (Evaluate potential moves) by enforcing a state with only one legal optimal move, thus checking adherence to rules ($\hyperref[FR.S.2.1]{FR.S.2.1}$) and logical constraints.

\textbf{How test will be performed:} An automated simulation within the $\RL{}$ Environment ($\hyperref[FR.S.2.1]{FR.S.2.1}$) is run 10 times from this exact pre-set state. All 10 runs must result in the suggestion of the single legal road placement.

\item{T.FR.AI.1.2\\}
\textbf{Control:} Manual

\textbf{Initial State:} $\GameState{}$ where Player A is 1 Victory Point (VP) away from winning (e.g., 9 VP + Longest Road) and has enough resources to build a settlement, but not a city. An open, unblocked node that grants the winning 10th VP is available.

\textbf{Input:} $\GameState{}$ where the immediate winning move is available.

\textbf{Output:} The $\AI{}$ must provide the suggestion to \texttt{BuildSettlement} at the winning node, with a high confidence score ($\hyperref[FR.S.3.4]{FR.S.3.4}$). The textual reasoning should explicitly state that this move secures the victory.

\textbf{Test Case Derivation:} Verifies $\hyperref[FR.S.3.1]{FR.S.3.1}$ (Strategy Prediction) by testing the $\AI{}$'s ability to identify and prioritize an immediate game-winning condition over resource maximization.

\textbf{How test will be performed:} Load the state and manually check the UI display ($\hyperref[FR.S.4.4]{FR.S.4.4}$) to ensure the winning move is highlighted and prioritized over all other options.

\item{T.FR.AI.1.3\\}
\textbf{Control:} Automatic

\textbf{Initial State:} $\GameState{}$ loaded for a 4-player game where the current player (Player A) has 6 resources and rolls a 7, triggering the Robber phase.

\textbf{Input:} The structured $\GameState{}$ is fed to the $\AI{}$ model.

\textbf{Output:} The $\AI{}$ must correctly calculate that Player A needs to discard 3 cards (half of their resources, rounded down), and must suggest a move (placing the Robber, stealing a card) that is fully legal, verifying the rule simulation ($\hyperref[FR.S.2.1]{FR.S.2.1}$).

\textbf{Test Case Derivation:} Checks a critical game transition rule (Robber movement/discarding) against the simulated environment and $\AI{}$ logic, ensuring adherence to the core ruleset.

\textbf{How test will be performed:} Run 10 simulations where the Robber is rolled. Verify the system logic correctly applies the discarding rule and the $\AI{}$ suggests a valid Robber placement move.
\end{enumerate}

\subsubsection{User Interface and Data Management}

This area verifies that the system correctly manages, displays, and archives game data, supporting both real-time user interaction ($\hyperref[FR.S.4.1]{FR.S.4.1}$ to $\hyperref[FR.S.4.6]{FR.S.4.6}$) and persistent data storage ($\hyperref[FR.S.5.1]{FR.S.5.1}$ to $\hyperref[FR.S.5.5]{FR.S.5.5}$). This ensures data integrity and user experience.

\textbf{Test Case: Real-time UI Update and Database Integrity}

\begin{enumerate}

\item{T.FR.UI.2.1\\}
\textbf{Control:} Manual/Automatic

\textbf{Initial State:} Game has just started. Player A has 2 settlements and 2 roads. The UI displays Player A's resource inventory as empty.

\textbf{Input:} Player A performs the action \texttt{BuildRoad} at node X, which is processed by the Game State Manager ($\hyperref[FR.S.7.4]{FR.S.7.4}$) and updated in the $\GameState{}$ Database ($\hyperref[FR.S.5.1]{FR.S.5.1}$) using the Image Queue ($\hyperref[FR.S.6.1]{FR.S.6.1}$).

\textbf{Output:}
    a) The UI must immediately reflect the change, showing the new road on the board visualization ($\hyperref[FR.S.4.1]{FR.S.4.1}$) and Player A's resource inventory accurately reduced ($\hyperref[FR.S.4.2]{FR.S.4.2}$).
    b) The $\GameState{}$ Database entry for this turn must be complete, correctly logging the pre-move state, the move performed, and the resulting post-move state ($\hyperref[FR.S.5.2]{FR.S.5.2}$).

\textbf{Test Case Derivation:} Verifies the real-time data flow requirements ($\hyperref[FR.S.6.1]{FR.S.6.1}$) and the system's ability to automatically and accurately update all components after a player action ($\hyperref[FR.S.7.4]{FR.S.7.4}$).

\textbf{How test will be performed:} Perform 5 consecutive build actions and measure the time for the UI to update. After the sequence, query the $\GameState{}$ Database to confirm that 5 distinct, consistent, and sequentially correct game states were logged ($\hyperref[FR.S.5.1]{FR.S.5.1}$).

\item{T.FR.UI.2.2\\}
\textbf{Control:} Automatic

\textbf{Initial State:} Game is 50 turns long and complete. The game state database contains the full history.

\textbf{Input:} Request historical game data ($\hyperref[FR.S.5.5]{FR.S.5.5}$) for the completed game, specifically requesting all dice rolls and all development card purchases.

\textbf{Output:} The system successfully returns the complete list of 50 turn records, including the dice roll for each turn, and a timestamped record of every development card purchased, demonstrating access to historical data ($\hyperref[FR.S.7.3]{FR.S.7.3}$).

\textbf{Test Case Derivation:} Verifies the requirements for historical logging and structured access to past game data ($\hyperref[FR.S.5.5]{FR.S.5.5}$).

\textbf{How test will be performed:} An automated script runs 10 full simulated games, each over 40 turns. After each game, the script attempts to query the database for the complete turn log. The test passes only if all 10 logs are complete, consistent, and accurately reflect the game rules.

\end{enumerate}

\subsubsection{Computer Vision and Game State Capture}

This area focuses on verifying the accuracy and reliability of the Computer Vision (\CV{}) Model ($\FR{}$.S.1.x) and the Game State Manager's ($\FR{}$.S.7.x) ability to convert visual input into a consistent digital representation ($\GameState{}$). The tests ensure the system can accurately detect board elements ($\hyperref[FR.S.1.1]{FR.S.1.1}$), translate them into structured data ($\hyperref[FR.S.1.2]{FR.S.1.2}$), and maintain synchronization ($\hyperref[FR.S.7.1]{FR.S.7.1}$).

\textbf{Test Case: Initial Board Setup Recognition}

\begin{enumerate}

\item{T.FR.CV.3.1\\}
\textbf{Control:} Manual

\textbf{Initial State:} A standard, legal $\Catan{}$ board is set up for the initial placement phase. Resource tiles, number tokens, and the Robber are placed according to the rules. No settlements or roads have been placed.

\textbf{Input:} A captured image (or video frame) of the physical board.

\textbf{Output:} A structured $\GameState{}$ data object where the board topology (tile types and locations, number tokens, Robber position) is correctly mapped to the digital twin. The asset counts for all players (settlements: 5, roads: 15) must be correct, verifying $\hyperref[FR.S.7.2]{FR.S.7.2}$.

\textbf{Test Case Derivation:} Directly verifies $\hyperref[FR.S.1.2]{FR.S.1.2}$ (Feature-to-State Translation) and $\hyperref[FR.S.7.1]{FR.S.7.1}$ (State Synchronization) using a known baseline state.

\textbf{How test will be performed:} A set of 5 different, legally generated board setups will be used. For each setup, the system is fed the image, and the output $\GameState{}$ dictionary is inspected to ensure a 100\% match with the actual setup.

\item{T.FR.CV.3.2\\}
\textbf{Control:} Manual

\textbf{Initial State:} A game is mid-play. Player A has 2 settlements, 1 city, 8 roads. The camera is slightly misaligned, causing a minor occlusion of one edge.

\textbf{Input:} A captured image of the board with the occlusion.

\textbf{Output:} The system successfully detects the presence of Player A's 2 settlements, 1 city, and 8 roads, correctly applying logical constraints to infer the state despite the occlusion. The system provides a diagnostic confidence metric ($\hyperref[FR.S.1.5]{FR.S.1.5}$) for the occluded area.

\textbf{Test Case Derivation:} Verifies $\hyperref[FR.S.1.3]{FR.S.1.3}$ (Error Detection and Correction) and $\hyperref[FR.S.1.5]{FR.S.1.5}$ (Diagnostic Feedback) under sub-optimal real-world conditions.

\textbf{How test will be performed:} Artificially introduce a small, known occlusion (e.g., a hand partially covering an intersection) in a mid-game state. Check that the digital state is correctly reconstructed and that the confidence metric correctly flags the occluded element as low-confidence/corrected.


\item{T.FR.CV.3.3\\}
\textbf{Control:} Manual

\textbf{Initial State:} A standard, legal $\Catan{}$ board is set up.

\textbf{Input:} A captured image of the physical board is processed by the $\CV{}$ Model. One piece (e.g., a single road) is intentionally misidentified or missed by the system.

\textbf{Output:}
a) The system must present the parsed board state to the user for confirmation, as required by $\hyperref[FR.Sa.1]{FR.Sa.1}$\textsuperscript{}.
b) The user must be able to successfully use the interface to manually correct the error (e.g., click to add the missing road or change the piece type) before proceeding.
c) The $\GameState{}$ Manager must use the \textit{corrected} state, not the original erroneous scan.

\textbf{Test Case Derivation:} Directly verifies the hazard mitigation requirement $\hyperref[FR.Sa.1]{FR.Sa.1}$\textsuperscript{}.

\textbf{How test will be performed:} An image known to cause a minor CV error will be used. The tester will follow the UI prompts to manually correct the board state and confirm the correction is accepted by the system.

\end{enumerate}


\subsection{Traceability Between Test Cases and Requirements}

The table below documents the explicit traceability between the defined system test cases ($\mathit{T.ID}$) and the specific requirements ($\mathit{Req.ID}$) they verify. This ensures that every functional and critical non-functional requirement is covered with appropriate redundancy, building confidence in the product if all tests pass.

\begin{table}[h!]
    \centering
    \caption{Functional Requirement Test Case Traceability Matrix}
    \label{tab:fr-system-test-traceability}
    \begin{tabularx}{\textwidth}{|c|X|}
    \hline
    \textbf{Test Case ID} & \textbf{Requirements Verified (Subset)} \\
    \hline
    T.FR.AI.1.1 & $\hyperref[FR.S.3.1]{FR.S.3.1}$, $\hyperref[FR.S.3.2]{FR.S.3.2}$, $\hyperref[FR.S.2.1]{FR.S.2.1}$, $\hyperref[FR.S.2.5]{FR.S.2.5}$, $\hyperref[FR.Sa.3]{FR.Sa.3}$ \\
    \hline
    T.FR.AI.1.2 & $\hyperref[FR.S.3.1]{FR.S.3.1}$, $\hyperref[FR.S.3.4]{FR.S.3.4}$ \\
    \hline
    T.FR.AI.1.3 & $\hyperref[FR.S.2.1]{FR.S.2.1}$ \\
    \hline
    T.FR.UI.2.1 & $\hyperref[FR.S.4.1]{FR.S.4.1}$, $\hyperref[FR.S.4.2]{FR.S.4.2}$, $\hyperref[FR.S.5.1]{FR.S.5.1}$, $\hyperref[FR.S.5.2]{FR.S.5.2}$, $\hyperref[FR.S.6.1]{FR.S.6.1}$, $\hyperref[FR.S.7.4]{FR.S.7.4}$, $\hyperref[FR.S.7.2]{FR.S.7.2}$ \\
    \hline
    T.FR.UI.2.2 & $\hyperref[FR.S.5.5]{FR.S.5.5}$, $\hyperref[FR.S.7.3]{FR.S.7.3}$ \\
    \hline
    T.FR.CV.3.1 & $\hyperref[FR.S.1.2]{FR.S.1.2}$, $\hyperref[FR.S.7.1]{FR.S.7.1}$, $\hyperref[FR.S.7.2]{FR.S.7.2}$ \\
    \hline
    T.FR.CV.3.2 & $\hyperref[FR.S.1.3]{FR.S.1.3}$, $\hyperref[FR.S.1.5]{FR.S.1.5}$ \\
    \hline
    T.FR.CV.3.3 & $\hyperref[FR.Sa.1]{FR.Sa.1}$, $\hyperref[FR.S.7.4]{FR.S.7.4}$ \\
    \hline
    \end{tabularx}
\end{table}

\section{Tests for Nonfunctional Requirements}

This section outlines the system-level tests for validating the nonfunctional requirements (NFRs) defined in Section~S.2.8 of the SRS. These include scalability, usability, maintainability, installability, data integrity, and availability. 

Each subsection corresponds to a major nonfunctional quality attribute of the RLCatan system. Some tests produce summary statistics or qualitative evaluations (e.g., graphs or surveys) rather than strict pass/fail outcomes.  

Accuracy-related nonfunctional qualities are validated implicitly through the functional tests defined in Section~4.2 (T.FR.AI.2.1--2.3 and T.FR.CV.1.1--1.2), which record relative error and correctness metrics.

\subsection{Performance and Scalability}

This area ensures the RLCatan system maintains acceptable responsiveness and throughput when supporting multiple concurrent games.

\textbf{Test Case: Scalability and Response Time Evaluation}

\begin{enumerate}
\item T.NFR.P.1 \\
\textbf{Control:} Automatic

\textbf{Initial State:} The RLCatan server is running the complete software stack ($\AI{}$, $\CV{}$, Database, UI).

\textbf{Input/Condition:} Simulate 1, 3, and 5 concurrent games using the $\RL{}$ environment and mock UI clients.

\textbf{Output/Result:} Average response time per game, CPU load, and memory usage are measured and summarized in a graph.

\textbf{Test Case Derivation:} Verifies NFR.S.1 (Scalability) by demonstrating that response time increases sub-linearly with load.

\textbf{How test will be performed:} Automated load-testing scripts measure latency between GameState updates and $\AI{}$ recommendations. Results are summarized in a table of performance metrics versus number of sessions.
\end{enumerate}

\subsection{Usability}

This area focuses on confirming that the interface is intuitive and accessible to users with minimal training. It also includes accessibility checks for users with visual impairments.

\textbf{Test Case: Usability Survey and User Feedback}

\begin{enumerate}
\item T.NFR.U.1 \\
\textbf{Control:} Manual

\textbf{Initial State:} Final prototype deployed in a browser.

\textbf{Input/Condition:} 5--10 participants familiar with $\Catan{}$ complete a guided session and fill out the usability survey (Appendix 7.2).

\textbf{Output/Result:} Average rating $\geq 4/5$ for clarity, ease of navigation, and understanding of $\AI{}$ feedback.

\textbf{Test Case Derivation:} Verifies NFR.S.2 (Usability) through direct user evaluation.

\textbf{How test will be performed:} Conducted during the final demo; results aggregated and visualized as a bar chart.
\end{enumerate}

\textbf{Test Case: Accessibility and Visual Clarity}

\begin{enumerate}
\item T.NFR.U.2 \\
\textbf{Control:} Manual

\textbf{Initial State:} RLCatan UI loaded in browser.

\textbf{Input/Condition:} Apply color-blindness filters and vary display scaling.

\textbf{Output/Result:} All critical UI elements remain legible; contrast ratio $\geq 4.5:1$.

\textbf{Test Case Derivation:} Verifies accessibility and clarity under varied visual conditions.

\textbf{How test will be performed:} Tested using Chrome DevTools accessibility simulator; screenshots recorded for documentation.
\end{enumerate}

\subsection{Maintainability}

This area ensures the modular design and structure of the codebase support efficient updates and debugging.

\textbf{Test Case: Code Quality and Modular Design Inspection}

\begin{enumerate}
\item T.NFR.M.1 \\
\textbf{Control:} Static

\textbf{Initial State:} Complete source code committed to GitHub.

\textbf{Input/Condition:} Run \texttt{SonarQube} and \texttt{flake8} to generate maintainability and style reports.

\textbf{Output/Result:} No high-severity issues; modules follow consistent structure and clear separation of concerns.

\textbf{Test Case Derivation:} Verifies NFR.S.3 (Maintainability) through code inspection and static analysis.

\textbf{How test will be performed:} Peer inspection led by group members; issues logged in GitHub for resolution.
\end{enumerate}

\subsection{Installability}

This area validates that RLCatan installs and operates correctly across all supported operating systems and browsers.

\textbf{Test Case: Cross-Platform Installation Validation}

\begin{enumerate}
\item T.NFR.I.1 \\
\textbf{Control:} Manual

\textbf{Initial State:} Packaged release available.

\textbf{Input/Condition:} Install and execute RLCatan on Windows~11 and macOS using Chrome and Firefox browsers.

\textbf{Output/Result:} Application installs and runs successfully on all tested platforms.

\textbf{Test Case Derivation:} Verifies NFR.S.4 (Installability) by ensuring cross-platform compatibility.

\textbf{How test will be performed:} Each team member installs on a unique OS following a checklist; issues recorded and documented.
\end{enumerate}

\subsection{Data Integrity}

This area ensures that the Game State Database maintains consistent, valid, and accurate data under all conditions, including normal operation, concurrent updates, and unexpected failures. This includes validation of incoming updates and transactional consistency to prevent corruption.

\textbf{Test Case: Transaction Integrity and Recovery}

\begin{enumerate}
\item T.NFR.D.1 \\
\textbf{Control:} Automatic

\textbf{Initial State:} Active game session connected to the database.

\textbf{Input/Condition:} Execute 10 consecutive state updates with an artificial network interruption during one update.

\textbf{Output/Result:} Database remains consistent; interrupted transaction is rolled back cleanly; no other game states are affected.

\textbf{Test Case Derivation:} Verifies transactional consistency portion of NFR.S.5 under failure conditions.

\textbf{How test will be performed:} Automated integration test simulates network failures and verifies database consistency after each run.
\end{enumerate}

\textbf{Test Case: Data Validation on Updates}

\begin{enumerate}
\item T.NFR.D.2 \\
\textbf{Control:} Automatic

\textbf{Initial State:} Active game session connected to the database.

\textbf{Input/Condition:} Attempt to submit invalid game state updates, such as negative resource counts, illegal moves, or missing required fields.

\textbf{Output/Result:} Invalid updates are recognized and rejected.

\textbf{Test Case Derivation:} Verifies validation portion of NFR.S.5, preventing data corruption from malformed inputs.

\textbf{How test will be performed:} Automated tests inject invalid game state objects and verifies rejection.
\end{enumerate}

\textbf{Test Case: Concurrent Update Handling}

\begin{enumerate}
\item T.NFR.D.3 \\
\textbf{Control:} Automatic

\textbf{Initial State:} Two or more simultaneous game sessions active.

\textbf{Input/Condition:} Simulate concurrent updates to overlapping resources (e.g., two players updating the same tile).

\textbf{Output/Result:} Only one valid transaction is committed; conflicts are detected and resolved; no data corruption occurs.

\textbf{Test Case Derivation:} Verifies database integrity under concurrent operations, completing NFR.S.5 coverage.

\textbf{How test will be performed:} Automated integration tests run multiple simulated clients updating shared resources; system logs and database state are verified for correctness.
\end{enumerate}


\subsection{Availability}

This area verifies that the system automatically recovers from failures within one minute, as required in the SRS.

\textbf{Test Case: System Failure and Auto-Recovery}

\begin{enumerate}
\item T.NFR.AV.1 \\
\textbf{Control:} Manual

\textbf{Initial State:} System running with active game session.

\textbf{Input/Condition:} Force a server crash or database disconnect mid-turn.

\textbf{Output/Result:} System restarts and restores previous game state within 60~seconds.

\textbf{Test Case Derivation:} Verifies NFR.S.6 (Availability) by measuring recovery time following induced failure.

\textbf{How test will be performed:} Use shell script to crash the process, record downtime using system logs, verify recovery from last snapshot.
\end{enumerate}

\subsection{Accuracy and Correctness (Referenced)}

Accuracy-related NFRs are verified by functional tests defined in Section~4.2:
\begin{itemize}
    \item T.FR.AI.2.1--2.3 ($\AI{}$ Model and Recommendation Logic)
    \item T.FR.CV.1.1--1.2 (Computer Vision and Game State Capture)
\end{itemize}
These tests report relative error between predicted and expected outcomes. No separate nonfunctional test cases are required.

\subsection{Traceability Between Test Cases and Requirements }

\begin{table}[H]
  \centering
  \caption{Non-Functional Requirement Test Case Traceability Matrix}
  \label{tab:nfr-system-test-traceability}
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Test Case ID} & \textbf{Requirement(s) Verified} \\ \hline
    T.NFR.P.1   & NFR.S.1 -- Scalability \\ \hline
    T.NFR.U.1   & NFR.S.2 -- Usability \\ \hline
    T.NFR.U.2   & NFR.S.2 -- Accessibility \\ \hline
    T.NFR.M.1   & NFR.S.3 -- Maintainability \\ \hline
    T.NFR.I.1   & NFR.S.4 -- Installability \\ \hline
    T.NFR.D.1   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.D.2   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.D.3   & NFR.S.5 -- Data Integrity \\ \hline
    T.NFR.AV.1  & NFR.S.6 -- Availability \\ \hline
  \end{tabular}
\end{table}


\section{Unit Test Description}
This section is to be completed after the MIS (detailed design document) has been completed.

\begin{comment}
\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
\end{comment}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.


\begin{table}[h!]
    \centering
    \caption{Symbolic Constants for System Tests}
    \label{tab:symbolic-parameters}
    \begin{tabularx}{\textwidth}{|c|p{2.0cm}|X|}
    \hline
    \textbf{Constant} & \textbf{Value} & \textbf{Description} \\
    \hline
    $T\_\text{RESP}$ & 5 seconds & Maximum allowed time for the $\AI{}$ to display a move recommendation ($\hyperref[NFR.E.4]{NFR.E.4}$, $\hyperref[NFR.Sa.1]{NFR.Sa.1}$). \\
    \hline
    $N\_\text{BATCH}$ & 100 & Number of $\GameState{}$ objects used in a batch test for latency measurement. \\
    \hline
    $N\_\text{RECONNECT}$ & 3 & Minimum number of automatic reconnection attempts after connection loss ($\hyperref[FR.Sa.6]{FR.Sa.6}$). \\
    \hline
    $N\_\text{BOARD\_SETUP}$ & 5 & Number of legally generated board setups used to test CV initialization. \\
    \hline
    $N\_\text{SIM\_RUNS}$ & 10 & Number of simulation runs for verifying $\RL{}$ move validation and rule application. \\
    \hline
    \end{tabularx}
\end{table}



\subsection{Usability Survey Questions?}


This section provides the planned questions for the usability survey, which will be used to validate the usability Non-Functional Requirements ($\hyperref[NFR.S.2]{NFR.S.2}$) and gather feedback on the clarity of the $\AI{}$ suggestions ($\hyperref[FR.Sa.7]{FR.Sa.7}$), primarily targeting the Novice and Intermediate player profiles.

\begin{enumerate}
    \item \textbf{Clarity of $\AI{}$ Suggestion:} When the $\AI{}$ suggested a move, how clear was the visual highlighting or overlay that indicated the location and action? (1=Very Unclear, 5=Very Clear)
    \item \textbf{Ease of Interface Use:} How easy was it to navigate the RLCatan interface to access the current $\GameState{}$ and receive the $\AI{}$ advice? (1=Very Difficult, 5=Very Easy)
    \item \textbf{Speed of Advice:} Did the advice appear fast enough to be useful during a normal turn of play? (Yes/No, If No: please explain.)
    \item \textbf{Trust in System:} How much confidence do you have in the system's ability to accurately reflect the physical board state? (1=Very Low, 5=Very High)
    \item \textbf{Usefulness of Post-Game Analysis:} If you used the post-game analysis, how helpful were the insights for improving your future strategy? (1=Not Helpful, 5=Extremely Helpful)
\end{enumerate}


\newpage{}
\section*{Appendix --- Reflection}


The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\subsection*{Jake Read}
\begin{enumerate}
    \item This deliverable went fairly smoothly, mostly because we already had a clear set of requirements from the SRS to base the tests on.
    Unlike the previous deliverables, we split into two subteams.
    Matthew and Rebecca worked primarily on this doc, while Sunny and I focused on getting started with the PoC, and then checked in with the VnV team to help review.
    I feel like this made the process more efficient, and less people working directly on one document meant fewer consistency issues.
    \item Personally, I didn't have any major pain points with the VnV plan, but as I said before, I was mainly focused on the PoC and simply reviewed and helped organize VnV progress.
    I think Rebecca and Matthew did a great job, so the review process was quite smooth.
    It's possible they had some pain points I'm not aware of, but from my discussions with them this doc was more straightforward than the SRS.
    \item For VnV, I think the team will need to acquire more knowledge about automated testing frameworks, particularly for Python and JavaScript.
    I'm not sure how experienced the others are in these areas, but when it comes to testing almost all my experience is in Java, so I think this will be a learning opportunity for me.
    We'll also need to familiarize ourselves with static analysis tools like SonarQube, as well as CI/CD pipelines using GitHub Actions.
    I can focus on SonarQube, since I'm interested in using it for future projects.
    \item For learning about automated testing frameworks, we could either take online courses/tutorials or read official documentation and implement small projects to practice.
    I also previously used SonarQube briefly back in second year, so I could revisit that experience and supplement it with online resources.
    Of these options, I'll start with some online tutorials, and then reference what I did with some of my old work.
\end{enumerate}

\subsection*{Rebecca DiFilippo}
\begin{enumerate}
    \item Writing this deliverable went fairly smoothly, mainly because we had a solid set of
    requirements from the SRS to guide the tests. Splitting into two subteams worked really well
     this time. Matthew and I focused on drafting the V\&V document, while Sunny and Jake worked
     on the PoC and then helped review our work. I think this setup made things more organized and
      helped avoid overlapping edits and consistency issues. It also allowed us to make progress
      on the POC plan.

    \item I didn’t run into major pain points, though coordinating between the two subteams
     sometimes required a bit of back-and-forth to make sure everyone was aligned. Overall, Matthew
      and I were able to keep the doc consistent, and Jake and Sunny’s feedback was really helpful
       for catching small issues early. The process felt much smoother than working on the SRS.

    \item For V\&V, the team will need to build more experience with automated testing frameworks.We also we’ll need to
       get comfortable with static analysis tools like SonarQube and CI/CD pipelines through GitHub
        Actions. I’ll focus on automated testing in Python since that’s an area I want to strengthen
         for this project and future work which includes flake8 style checks.

    \item To build these skills, we can look for online  tutorials and courses, as well as consult
      with other people who have experience in these areas. I’ll start with online tutorials for
       Python testing frameworks and flake8, as well as look for example projects that implement
        these tools effectively.
\end{enumerate}

\end{document}